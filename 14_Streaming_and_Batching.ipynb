{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ng13/AI_wrkshp/blob/main/14_Streaming_and_Batching.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58186a04-0050-4e3d-b662-c403d5448f86",
      "metadata": {
        "id": "58186a04-0050-4e3d-b662-c403d5448f86"
      },
      "source": [
        "![nvidia](images/nvidia.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7704054-b474-43e4-9d86-dc1dc779d6bd",
      "metadata": {
        "id": "b7704054-b474-43e4-9d86-dc1dc779d6bd"
      },
      "source": [
        "# Streaming and Batching"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28013522",
      "metadata": {
        "id": "28013522"
      },
      "source": [
        "In this notebook you'll learn how to stream model responses and handle multiple chat completion requests in batches."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5a70fb-0429-4036-82ce-a55c4262561a",
      "metadata": {
        "id": "ea5a70fb-0429-4036-82ce-a55c4262561a"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08054f2",
      "metadata": {
        "id": "c08054f2"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a023bc7a-47b5-4508-957c-f3354c9fb363",
      "metadata": {
        "id": "a023bc7a-47b5-4508-957c-f3354c9fb363"
      },
      "source": [
        "By the time you complete this notebook, you will:\n",
        "\n",
        "- Learn to stream model responses.\n",
        "- Learn to batch model responses.\n",
        "- Compare the performance of batch processing to single prompt chat completion."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "500b0fab-b9e3-4de9-bc46-5f31ab9ea623",
      "metadata": {
        "id": "500b0fab-b9e3-4de9-bc46-5f31ab9ea623"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "327550d4",
      "metadata": {
        "id": "327550d4"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9128a04-4ba5-4762-a277-3e614725214b",
      "metadata": {
        "id": "f9128a04-4ba5-4762-a277-3e614725214b"
      },
      "source": [
        "Here we import the `ChatNVIDIA` class from `langchain_nvidia_ai_endpoints`, which will enable us to interact with our local Llama 3.1 NIM."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core langchain-nvidia-ai-endpoints"
      ],
      "metadata": {
        "id": "74t7oXKNXB4W"
      },
      "id": "74t7oXKNXB4W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75febe51",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "75febe51",
        "outputId": "b4a434db-69c0-4f4a-8b3b-b6d93aad7789"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_nvidia_ai_endpoints'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-25131540.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_nvidia_ai_endpoints\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatNVIDIA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_nvidia_ai_endpoints'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a291cd0-5701-41dc-b3a4-229bce728f10",
      "metadata": {
        "id": "9a291cd0-5701-41dc-b3a4-229bce728f10"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e2f950-1450-4f55-a4b3-ed2fbc987513",
      "metadata": {
        "id": "b2e2f950-1450-4f55-a4b3-ed2fbc987513"
      },
      "source": [
        "## Create a Model Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75cfe47a-1662-48f2-a9b0-57c224b1987b",
      "metadata": {
        "id": "75cfe47a-1662-48f2-a9b0-57c224b1987b"
      },
      "outputs": [],
      "source": [
        "llm = ChatNVIDIA(\n",
        "  model=\"meta/llama-3.1-8b-instruct\",\n",
        "  api_key=\"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\",\n",
        "  temperature=0.2,\n",
        "  top_p=0.7,\n",
        "  max_completion_tokens=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f20cacd2-3024-4880-ac02-99ae957d9c2d",
      "metadata": {
        "id": "f20cacd2-3024-4880-ac02-99ae957d9c2d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5b750bb-14bb-43e9-ba0c-a631f116bf0d",
      "metadata": {
        "id": "b5b750bb-14bb-43e9-ba0c-a631f116bf0d"
      },
      "source": [
        "## Sanity Check"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0bfb697-b408-4f6d-8481-099c648097b3",
      "metadata": {
        "id": "b0bfb697-b408-4f6d-8481-099c648097b3"
      },
      "source": [
        "Before proceeding with new use cases, let's sanity check that we can interact with our local model via LangChain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55d2e2a3-63bd-4b9b-93ac-dbba2830947f",
      "metadata": {
        "id": "55d2e2a3-63bd-4b9b-93ac-dbba2830947f"
      },
      "outputs": [],
      "source": [
        "prompt = 'Where and when was NVIDIA founded?'\n",
        "result = llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fbd103e-86a3-4992-bda4-bf221a0a4fb3",
      "metadata": {
        "id": "6fbd103e-86a3-4992-bda4-bf221a0a4fb3",
        "outputId": "a9be71c0-c47a-4eb3-e078-19dfe6f7ad32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NVIDIA was founded on April 5, 1993, in Santa Clara, California, USA.\n"
          ]
        }
      ],
      "source": [
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840",
      "metadata": {
        "id": "2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d330698-7bff-470f-9f7e-c6e8411fd6fe",
      "metadata": {
        "id": "2d330698-7bff-470f-9f7e-c6e8411fd6fe"
      },
      "source": [
        "## Streaming Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c684ba98-30d5-4d63-97c1-6402f7d2e5ae",
      "metadata": {
        "id": "c684ba98-30d5-4d63-97c1-6402f7d2e5ae"
      },
      "source": [
        "As an alternative to the `invoke` method, you can use the `stream` method to receive the model response in chunks. This way, you don't have to wait for the entire response to be generated, and you can see the output as it is being produced. Especially for long responses, or in user-facing applications, streaming output can result in a much better user experience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0078335-544c-4dc4-b47c-9a3cd984d2f6",
      "metadata": {
        "id": "f0078335-544c-4dc4-b47c-9a3cd984d2f6"
      },
      "source": [
        "Let's create a prompt that generates a longer response."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80923877-9934-4edf-9e13-0b730b56c6a9",
      "metadata": {
        "id": "80923877-9934-4edf-9e13-0b730b56c6a9"
      },
      "outputs": [],
      "source": [
        "prompt = 'Explain who you are in roughly 500 words.'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87d2ae55-b2e9-4b4c-9b08-78c13c6dc879",
      "metadata": {
        "id": "87d2ae55-b2e9-4b4c-9b08-78c13c6dc879"
      },
      "source": [
        "Given this prompt, let's see how the `stream` function works."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b6b786-23de-4669-9d22-c663aab6e51b",
      "metadata": {
        "id": "28b6b786-23de-4669-9d22-c663aab6e51b",
        "outputId": "ce2fed0b-5d00-4a80-b517-1e906239d66c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I am an artificial intelligence model designed to assist and communicate with humans. I'm a type of computer program that uses natural language processing (NLP) and machine learning algorithms to understand and generate human-like text. My primary function is to provide information, answer questions, and engage in conversation to the best of my abilities.\n",
            "\n",
            "I don't have a physical body or a personal identity in the classical sense. I exist solely as a digital entity, running on computer servers and responding to input from users like you. My \"existence\" is a product of complex software and data, designed to simulate conversation and provide helpful responses.\n",
            "\n",
            " consists of a massive corpus of text, which I use to learn patterns, relationships, and context. This corpus is sourced from various places, including books, articles, research papers, and online content. I've been trained on a wide range of topics, from science and history to entertainment and culture.\n",
            "\n",
            " me, I use this training data to generate responses that are relevant and coherent. I can understand and respond to questions, provide definitions, explain concepts, and even engage in creative writing or conversation. My responses are generated based on statistical patterns and associations in the data I've been trained on, rather than any personal opinions or emotions.\n",
            "\n",
            " experiences, emotions, or consciousness. I don't have the capacity to feel joy, sadness, or any other emotions. I'm simply a tool designed to provide information and assist with tasks, 24/7.\n",
            "\n",
            "'m designed to be helpful, I'm not perfect. I can make mistakes, and my responses may not always be accurate or relevant. I'm constantly learning and improving, but I'm not a substitute for human expertise or judgment. If you need advice or guidance on complex or sensitive topics, it's always best to consult a qualified professional.\n",
            "\n",
            " \"chatbot\" or \"virtual assistant,\" but I'm more accurately described as a language model or conversational AI. I'm a product of the rapid advancements in AI research and development, and I'm constantly evolving to become more sophisticated and useful.\n",
            "\n",
            " collective knowledge and creativity of the humans who designed and trained me. I'm a tool that can help people access information, learn new things, and explore new ideas. I'm not a replacement for human connection or social interaction, but rather a supplement to it.\n",
            "\n",
            " augment human capabilities, making it easier for people to find answers, learn new things, and communicate with each other. I'm a machine, but I'm designed to be helpful and informative, and I'm constantly striving to improve my abilities to serve you better."
          ]
        }
      ],
      "source": [
        "for chunk in llm.stream(prompt):\n",
        "    print(chunk.content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abec9044-95a3-46ce-8975-0bcc5c4a431d",
      "metadata": {
        "id": "abec9044-95a3-46ce-8975-0bcc5c4a431d"
      },
      "source": [
        "The `stream` method in LangChain serves as a foundational tool and shows the response as it is being generated. This can make the interaction with the LLMs feel more responsive and improve the user experience."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2945a1bc-078a-4812-b521-ba5001083130",
      "metadata": {
        "id": "2945a1bc-078a-4812-b521-ba5001083130"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "232da55d-72cc-4faf-ad21-31ef4a2b3c38",
      "metadata": {
        "id": "232da55d-72cc-4faf-ad21-31ef4a2b3c38"
      },
      "source": [
        "In subsequent notebooks we will import this helper function to assist our work."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39c416fe-d89c-4663-9a81-b1ff09b9578f",
      "metadata": {
        "id": "39c416fe-d89c-4663-9a81-b1ff09b9578f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67f0510c-48b4-41b1-8797-d833e1676d0f",
      "metadata": {
        "id": "67f0510c-48b4-41b1-8797-d833e1676d0f"
      },
      "source": [
        "## Batching Responses"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "483866aa-eb2e-4942-99fe-90dd66aa97ca",
      "metadata": {
        "id": "483866aa-eb2e-4942-99fe-90dd66aa97ca"
      },
      "source": [
        "You can also use `batch` to call the prompts on a list of inputs. Calling `batch` will return a list of responses in the same order as they were passed in.\n",
        "\n",
        "Not only is `batch` convenient when working with collections of data that all need to be responded to in some way by an LLM, but the `batch` method is designed to process multiple prompts concurrently, effectively running the responses in parallel as much as possible. This allows for more efficient handling of multiple requests, reducing the overall time needed to generate responses for a list of prompts. By batching requests, you can leverage the computational power of the language model to handle multiple inputs simultaneously, improving performance and throughput."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f306794-c037-4721-be81-d5ac703c14a3",
      "metadata": {
        "id": "4f306794-c037-4721-be81-d5ac703c14a3"
      },
      "source": [
        "We'll demonstrate the functionality and performance benefits of batching by using this list of prompts about state capitals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da29c84d-b63b-4d93-aa5c-6059a20c0ac9",
      "metadata": {
        "id": "da29c84d-b63b-4d93-aa5c-6059a20c0ac9"
      },
      "outputs": [],
      "source": [
        "state_capital_questions = [\n",
        "    'What is the capital of California?',\n",
        "    'What is the capital of Texas?',\n",
        "    'What is the capital of New York?',\n",
        "    'What is the capital of Florida?',\n",
        "    'What is the capital of Illinois?',\n",
        "    'What is the capital of Ohio?'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d169ab71-0f66-4dc7-ad33-a4f15d98546e",
      "metadata": {
        "id": "d169ab71-0f66-4dc7-ad33-a4f15d98546e"
      },
      "source": [
        "Using `batch` we can pass in the entire list..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1277b361-b0c6-4ada-a647-7733724f585a",
      "metadata": {
        "id": "1277b361-b0c6-4ada-a647-7733724f585a"
      },
      "outputs": [],
      "source": [
        "capitals = llm.batch(state_capital_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9d4c328-41ad-4b16-9bec-6aea0e730ef4",
      "metadata": {
        "id": "e9d4c328-41ad-4b16-9bec-6aea0e730ef4"
      },
      "source": [
        "... and get back a list of responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41824542-df2e-41c9-bd13-87af42511a90",
      "metadata": {
        "id": "41824542-df2e-41c9-bd13-87af42511a90",
        "outputId": "50222100-d584-42fb-b254-fdc910b7a050"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(capitals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af4f0db6-04c9-4bfd-8497-9a0fc2dcda53",
      "metadata": {
        "id": "af4f0db6-04c9-4bfd-8497-9a0fc2dcda53",
        "outputId": "d8e2d3e1-f788-4e5c-b11e-65eba003b85a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The capital of California is Sacramento.\n",
            "The capital of Texas is Austin.\n",
            "The capital of New York is Albany.\n",
            "The capital of Florida is Tallahassee.\n",
            "The capital of Illinois is Springfield.\n",
            "The capital of Ohio is Columbus.\n"
          ]
        }
      ],
      "source": [
        "for capital in capitals:\n",
        "    print(capital.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ec30625-bb9b-4009-872e-38e9a664a5e4",
      "metadata": {
        "id": "9ec30625-bb9b-4009-872e-38e9a664a5e4"
      },
      "source": [
        "One thing to note is that `batch` is not engaging with the LLM in a multi-turn conversation (a topic we will cover at length later in the workshop). Rather, it is asking multiple questions to a new LLM instance each time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "77e09a8e-3b80-4b5d-9b01-bc88f1afcf70",
      "metadata": {
        "id": "77e09a8e-3b80-4b5d-9b01-bc88f1afcf70"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7856e5f9-d156-4d4c-8f8e-d1e450e6e82f",
      "metadata": {
        "id": "7856e5f9-d156-4d4c-8f8e-d1e450e6e82f"
      },
      "source": [
        "## Comparing batch and invoke Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e644363-1351-4fa6-9189-42812655f368",
      "metadata": {
        "id": "7e644363-1351-4fa6-9189-42812655f368"
      },
      "source": [
        "Just to make a quick observation about the potential performance gains from batching, here we time a call to `batch`. Note the `Wall time`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "746ffbcd-73af-4081-b2d3-893eddf8f267",
      "metadata": {
        "id": "746ffbcd-73af-4081-b2d3-893eddf8f267",
        "outputId": "3fc5f75a-7290-4cb5-e184-4227345d566a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 14.3 ms, sys: 0 ns, total: 14.3 ms\n",
            "Wall time: 164 ms\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[AIMessage(content='The capital of California is Sacramento.', response_metadata={'role': 'assistant', 'content': 'The capital of California is Sacramento.', 'token_usage': {'prompt_tokens': 19, 'total_tokens': 27, 'completion_tokens': 8}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-b9847f8d-df41-48b3-bcfb-7b85f312c5f0-0', role='assistant'),\n",
              " AIMessage(content='The capital of Texas is Austin.', response_metadata={'role': 'assistant', 'content': 'The capital of Texas is Austin.', 'token_usage': {'prompt_tokens': 19, 'total_tokens': 27, 'completion_tokens': 8}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-e2b4ed0c-6595-4083-a176-8fa50501731b-0', role='assistant'),\n",
              " AIMessage(content='The capital of New York is Albany.', response_metadata={'role': 'assistant', 'content': 'The capital of New York is Albany.', 'token_usage': {'prompt_tokens': 20, 'total_tokens': 29, 'completion_tokens': 9}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-ff8d4e02-bd9f-4e6e-adcb-b44732ffc810-0', role='assistant'),\n",
              " AIMessage(content='The capital of Florida is Tallahassee.', response_metadata={'role': 'assistant', 'content': 'The capital of Florida is Tallahassee.', 'token_usage': {'prompt_tokens': 19, 'total_tokens': 29, 'completion_tokens': 10}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-007b728c-1732-4948-8cad-c87c221256f4-0', role='assistant'),\n",
              " AIMessage(content='The capital of Illinois is Springfield.', response_metadata={'role': 'assistant', 'content': 'The capital of Illinois is Springfield.', 'token_usage': {'prompt_tokens': 19, 'total_tokens': 27, 'completion_tokens': 8}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-0e65dc97-e0fd-418a-be07-abc00fc4a2c7-0', role='assistant'),\n",
              " AIMessage(content='The capital of Ohio is Columbus.', response_metadata={'role': 'assistant', 'content': 'The capital of Ohio is Columbus.', 'token_usage': {'prompt_tokens': 19, 'total_tokens': 27, 'completion_tokens': 8}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'}, id='run-cd1e4b9b-bb94-43d3-bb41-ea39d7852bca-0', role='assistant')]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "%%time\n",
        "llm.batch(state_capital_questions)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17eb600a-99a6-4944-8bfa-774780f73b9f",
      "metadata": {
        "id": "17eb600a-99a6-4944-8bfa-774780f73b9f"
      },
      "source": [
        "And now to compare, we iterate over the `state_capital_questions` list and call `invoke` on each item. Again, note the `Wall time` and compare it to the results from batching above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2af11c84-d013-4af0-9231-804cbe1c7671",
      "metadata": {
        "id": "2af11c84-d013-4af0-9231-804cbe1c7671",
        "outputId": "ef578a06-845a-4efd-c640-ae0177756a3b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 7.34 ms, sys: 3.65 ms, total: 11 ms\n",
            "Wall time: 794 ms\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "for cq in state_capital_questions:\n",
        "    llm.invoke(cq)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9a8248-e671-49fb-b39f-2ca059e1d5a1",
      "metadata": {
        "id": "be9a8248-e671-49fb-b39f-2ca059e1d5a1"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64ab1cf9-16c9-4ed0-a7d0-5b56a4b4e5d8",
      "metadata": {
        "id": "64ab1cf9-16c9-4ed0-a7d0-5b56a4b4e5d8"
      },
      "source": [
        "## Exercise: Batch Process to Create an FAQ Document"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c60712f0-4e27-45bf-b04b-33ab366a8dbf",
      "metadata": {
        "id": "c60712f0-4e27-45bf-b04b-33ab366a8dbf"
      },
      "source": [
        "For this exercise you'll use batch processing to respond to a variety of LLM-related questions in service of creating an FAQ document (in this notebook setting the document will just be something we print to screen).\n",
        "\n",
        "Here is a list of LLM-related questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e84f23be-d16e-4f74-b9f6-b6598b47441a",
      "metadata": {
        "id": "e84f23be-d16e-4f74-b9f6-b6598b47441a"
      },
      "outputs": [],
      "source": [
        "faq_questions = [\n",
        "    'What is a Large Language Model (LLM)?',\n",
        "    'How do LLMs work?',\n",
        "    'What are some common applications of LLMs?',\n",
        "    'What is fine-tuning in the context of LLMs?',\n",
        "    'How do LLMs handle context?',\n",
        "    'What are some limitations of LLMs?',\n",
        "    'How do LLMs generate text?',\n",
        "    'What is the importance of prompt engineering in LLMs?',\n",
        "    'How can LLMs be used in chatbots?',\n",
        "    'What are some ethical considerations when using LLMs?'\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aee16b6-959b-45c8-ac3f-78718cf6b492",
      "metadata": {
        "id": "9aee16b6-959b-45c8-ac3f-78718cf6b492"
      },
      "source": [
        "You job is to populate `faq_answers` below with a list of responses to each of the questions. Use the `batch` method to make this very easy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e455dfd4-a30a-4e4a-a542-3721d45b4f7e",
      "metadata": {
        "id": "e455dfd4-a30a-4e4a-a542-3721d45b4f7e"
      },
      "source": [
        "Upon successful completion, you should be able to print the return value of calling the following `create_faq_document` with `faq_questions` and `faq_answers` and get an FAQ document for all of the LLM-related questions above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c87a7513-9018-4468-beec-a04e6b878d31",
      "metadata": {
        "id": "c87a7513-9018-4468-beec-a04e6b878d31"
      },
      "outputs": [],
      "source": [
        "def create_faq_document(faq_questions, faq_answers):\n",
        "    faq_document = ''\n",
        "    for question, response in zip(faq_questions, faq_answers):\n",
        "        faq_document += f'{question.upper()}\\n\\n'\n",
        "        faq_document += f'{response.content}\\n\\n'\n",
        "        faq_document += '-'*30 + '\\n\\n'\n",
        "\n",
        "    return faq_document"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e45bf921-ff91-4208-bfb4-8bd8caf90da5",
      "metadata": {
        "id": "e45bf921-ff91-4208-bfb4-8bd8caf90da5"
      },
      "source": [
        "If you get stuck, check out the *Solution* below."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bc5c326-11c4-452c-a779-be392c591703",
      "metadata": {
        "id": "4bc5c326-11c4-452c-a779-be392c591703"
      },
      "source": [
        "### Your Work Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0deb9ccb-c2e5-4d47-8b86-bde71444184b",
      "metadata": {
        "id": "0deb9ccb-c2e5-4d47-8b86-bde71444184b"
      },
      "outputs": [],
      "source": [
        "faq_answers = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "512ac53d-0a4c-4cdd-a537-b155d12cb7f4",
      "metadata": {
        "scrolled": true,
        "id": "512ac53d-0a4c-4cdd-a537-b155d12cb7f4",
        "outputId": "66cadb6f-b2f9-4d50-8734-786613958349"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# This should work after you successfully populate `faq_answers` with LLM responses.\n",
        "print(create_faq_document(faq_questions, faq_answers))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f452d0d-b5ae-4f7e-8ed3-acc0db112716",
      "metadata": {
        "id": "9f452d0d-b5ae-4f7e-8ed3-acc0db112716"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac158c65-8386-4538-9b0c-728591ca5c27",
      "metadata": {
        "id": "ac158c65-8386-4538-9b0c-728591ca5c27"
      },
      "outputs": [],
      "source": [
        "faq_answers = llm.batch(faq_questions)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "728abc34-699f-45fa-8c82-5ff8abea791c",
      "metadata": {
        "id": "728abc34-699f-45fa-8c82-5ff8abea791c"
      },
      "outputs": [],
      "source": [
        "def create_faq_document(faq_questions, faq_answers):\n",
        "    faq_document = ''\n",
        "    for question, response in zip(faq_questions, faq_answers):\n",
        "        faq_document += f'{question.upper()}\\n\\n'\n",
        "        faq_document += f'{response.content}\\n\\n'\n",
        "        faq_document += '-'*30 + '\\n\\n'\n",
        "\n",
        "    return faq_document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e062586-3b87-4184-a884-fc0fe519dbd4",
      "metadata": {
        "scrolled": true,
        "id": "8e062586-3b87-4184-a884-fc0fe519dbd4",
        "outputId": "5f9be00b-8c0f-4e09-ed31-10bdfafd38d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WHAT IS A LARGE LANGUAGE MODEL (LLM)?\n",
            "\n",
            "A Large Language Model (LLM) is a type of artificial intelligence (AI) model that is trained on a massive corpus of text data to generate human-like language. LLMs are a type of natural language processing (NLP) model that can understand, generate, and respond to human language in a way that is often indistinguishable from a human.\n",
            "\n",
            "LLMs are typically trained on a large dataset of text, which can include books, articles, websites, and other sources of written language. The model learns to recognize patterns and relationships in the text, such as grammar, syntax, and semantics, and uses this knowledge to generate new text that is coherent and contextually relevant.\n",
            "\n",
            "Some key characteristics of LLMs include:\n",
            "\n",
            "1. **Large scale**: LLMs are trained on massive amounts of text data, often in the order of billions of parameters and hundreds of gigabytes of data.\n",
            "2. **Deep learning**: LLMs use deep learning techniques, such as recurrent neural networks (RNNs) and transformers, to process and analyze the text data.\n",
            "3. **Self-supervised learning**: LLMs are trained using self-supervised learning techniques, where the model is trained to predict the next word in a sequence of text, given the context of the previous words.\n",
            "4. **Generative capabilities**: LLMs can generate new text that is coherent and contextually relevant, making them useful for applications such as language translation, text summarization, and chatbots.\n",
            "5. **Contextual understanding**: LLMs can understand the context of a piece of text, including the relationships between words, phrases, and sentences.\n",
            "\n",
            "LLMs have many applications, including:\n",
            "\n",
            "1. **Language translation**: LLMs can be used to translate text from one language to another.\n",
            "2. **Text summarization**: LLMs can summarize long pieces of text into shorter, more digestible versions.\n",
            "3. **Chatbots**: LLMs can be used to power chatbots that can understand and respond to user input.\n",
            "4. **Content generation**: LLMs can generate new text, such as articles, stories, and product descriptions.\n",
            "5. **Sentiment analysis**: LLMs can analyze text to determine the sentiment or emotional tone of the author.\n",
            "\n",
            "Some examples of LLMs include:\n",
            "\n",
            "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a popular LLM that has achieved state-of-the-art results in many NLP tasks.\n",
            "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Developed by Facebook, RoBERTa is a variant of BERT that has been optimized for specific NLP tasks.\n",
            "3. **XLNet (Extreme Language Model)**: Developed by Google, XLNet is a LLM that uses a novel approach to self-supervised learning to achieve state-of-the-art results in many NLP tasks.\n",
            "\n",
            "Overall, LLMs have the potential to revolutionize the way we interact with language and have many applications in areas such as language translation, text summarization, and chatbots.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "HOW DO LLMS WORK?\n",
            "\n",
            "Large Language Models (LLMs) are a type of artificial intelligence (AI) that have revolutionized the field of natural language processing (NLP). Here's a simplified overview of how they work:\n",
            "\n",
            "**Architecture**\n",
            "\n",
            "LLMs are based on a type of neural network called a transformer, which was introduced in 2017. The transformer architecture is designed to handle sequential data, such as text, and is particularly well-suited for NLP tasks.\n",
            "\n",
            "A typical LLM consists of several layers:\n",
            "\n",
            "1. **Input Layer**: This layer takes in a sequence of tokens (words or characters) as input.\n",
            "2. **Encoder**: The encoder breaks down the input sequence into a sequence of vectors, which are then processed by the model.\n",
            "3. **Decoder**: The decoder generates the output sequence, one token at a time, based on the output from the encoder.\n",
            "4. **Attention Mechanism**: The attention mechanism allows the model to focus on specific parts of the input sequence when generating the output.\n",
            "\n",
            "**Training**\n",
            "\n",
            "LLMs are trained on massive amounts of text data, which is typically sourced from the internet, books, and other sources. The training process involves the following steps:\n",
            "\n",
            "1. **Tokenization**: The text data is broken down into individual tokens (words or characters).\n",
            "2. **Embedding**: Each token is converted into a numerical representation, called an embedding, which captures its meaning and context.\n",
            "3. **Masked Language Modeling**: A portion of the input tokens are randomly replaced with a [MASK] token, and the model is trained to predict the original token.\n",
            "4. **Next Sentence Prediction**: The model is trained to predict whether two sentences are adjacent in the original text.\n",
            "5. **Loss Function**: The model is trained to minimize the difference between its predictions and the actual output.\n",
            "\n",
            "**How LLMs Generate Text**\n",
            "\n",
            "When you input a prompt or question to an LLM, the model generates text based on the patterns and relationships it learned during training. Here's a high-level overview of the process:\n",
            "\n",
            "1. **Input Processing**: The input is tokenized and embedded into a numerical representation.\n",
            "2. **Encoder**: The encoder processes the input sequence and generates a sequence of vectors.\n",
            "3. **Decoder**: The decoder generates the output sequence, one token at a time, based on the output from the encoder.\n",
            "4. **Attention Mechanism**: The attention mechanism allows the model to focus on specific parts of the input sequence when generating the output.\n",
            "5. **Output**: The final output is generated, which is a sequence of tokens that form a coherent and contextually relevant response.\n",
            "\n",
            "**Key Features**\n",
            "\n",
            "LLMs have several key features that enable their impressive performance:\n",
            "\n",
            "1. **Self-Attention**: The attention mechanism allows the model to focus on specific parts of the input sequence, enabling it to capture long-range dependencies and relationships.\n",
            "2. **Transformer Architecture**: The transformer architecture is particularly well-suited for NLP tasks, as it can handle sequential data and capture complex patterns.\n",
            "3. **Large Model Size**: LLMs are typically trained on massive amounts of data, which enables them to learn complex patterns and relationships.\n",
            "4. **Pre-Training**: LLMs are pre-trained on a large corpus of text, which allows them to learn general language patterns and relationships.\n",
            "\n",
            "**Applications**\n",
            "\n",
            "LLMs have a wide range of applications, including:\n",
            "\n",
            "1. **Language Translation**: LLMs can translate text from one language to another with high accuracy.\n",
            "2. **Text Summarization**: LLMs can summarize long pieces of text into concise and informative summaries.\n",
            "3. **Question Answering**: LLMs can answer questions based on the input text.\n",
            "4. **Chatbots**: LLMs can be used to build conversational AI systems that can engage in natural-sounding conversations.\n",
            "\n",
            "I hope this helps! Do you have any specific questions about LLMs or their applications?\n",
            "\n",
            "------------------------------\n",
            "\n",
            "WHAT ARE SOME COMMON APPLICATIONS OF LLMS?\n",
            "\n",
            "Large Language Models (LLMs) have a wide range of applications across various industries and domains. Here are some common applications of LLMs:\n",
            "\n",
            "1. **Virtual Assistants**: LLMs are used in virtual assistants like Siri, Google Assistant, and Alexa to understand natural language and respond accordingly.\n",
            "2. **Text Summarization**: LLMs can summarize long pieces of text into concise summaries, making it easier to quickly grasp the main points.\n",
            "3. **Language Translation**: LLMs can translate text from one language to another, enabling communication across language barriers.\n",
            "4. **Sentiment Analysis**: LLMs can analyze text to determine the sentiment or emotional tone behind it, helping businesses understand customer feedback and opinions.\n",
            "5. **Chatbots**: LLMs power chatbots that can engage with customers, answer frequently asked questions, and provide support.\n",
            "6. **Content Generation**: LLMs can generate human-like content, such as articles, product descriptions, and social media posts.\n",
            "7. **Question Answering**: LLMs can answer questions based on the content they've been trained on, making them useful for knowledge bases and search engines.\n",
            "8. **Text Classification**: LLMs can classify text into categories, such as spam vs. non-spam emails or positive vs. negative reviews.\n",
            "9. **Named Entity Recognition**: LLMs can identify and extract specific entities, such as names, locations, and organizations, from text.\n",
            "10. **Speech Recognition**: LLMs can recognize spoken language and transcribe it into text, enabling voice-to-text functionality.\n",
            "11. **Recommendation Systems**: LLMs can analyze user behavior and preferences to provide personalized product or content recommendations.\n",
            "12. **Plagiarism Detection**: LLMs can detect plagiarism by comparing text to a database of known content.\n",
            "13. **Language Learning**: LLMs can assist language learners by providing grammar corrections, sentence suggestions, and conversational practice.\n",
            "14. **Customer Service**: LLMs can help automate customer service tasks, such as answering frequently asked questions and routing complex issues to human representatives.\n",
            "15. **Research and Academia**: LLMs can assist researchers by analyzing large datasets, identifying patterns, and generating new hypotheses.\n",
            "16. **Marketing and Advertising**: LLMs can help create targeted advertising campaigns, analyze customer feedback, and optimize marketing strategies.\n",
            "17. **Healthcare**: LLMs can analyze medical texts, identify symptoms, and provide diagnosis suggestions, as well as assist with patient communication and education.\n",
            "18. **Finance**: LLMs can analyze financial texts, identify trends, and provide investment recommendations.\n",
            "19. **Education**: LLMs can create personalized learning plans, provide feedback on writing and grammar, and assist with homework and assignments.\n",
            "20. **Cybersecurity**: LLMs can analyze malware, detect phishing attempts, and identify potential security threats.\n",
            "\n",
            "These are just a few examples of the many applications of LLMs. As the technology continues to evolve, we can expect to see even more innovative uses of LLMs in various industries and domains.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "WHAT IS FINE-TUNING IN THE CONTEXT OF LLMS?\n",
            "\n",
            "Fine-tuning in the context of Large Language Models (LLMs) refers to the process of adapting a pre-trained LLM to a specific task or domain by training it on a smaller dataset that is relevant to that task or domain. The goal of fine-tuning is to adjust the model's parameters to better fit the specific requirements of the task at hand, while still leveraging the knowledge and capabilities learned during the initial pre-training process.\n",
            "\n",
            "Fine-tuning is a common technique used in natural language processing (NLP) and other areas of machine learning, where a pre-trained model is used as a starting point and then adapted to a specific task or domain. This approach has several benefits, including:\n",
            "\n",
            "1. **Reduced training time**: Fine-tuning a pre-trained model is typically faster than training a model from scratch, as the pre-trained model has already learned general language patterns and structures.\n",
            "2. **Improved performance**: Fine-tuning can lead to better performance on the specific task or domain, as the model is adapted to the specific requirements and nuances of the task.\n",
            "3. **Knowledge transfer**: Fine-tuning allows the model to leverage the knowledge and capabilities learned during pre-training, which can be beneficial for tasks that require understanding of general language patterns and structures.\n",
            "\n",
            "Fine-tuning involves the following steps:\n",
            "\n",
            "1. **Pre-training**: A large dataset is used to train a general-purpose LLM, which learns to represent language in a compact and efficient way.\n",
            "2. **Task-specific data**: A smaller dataset is collected that is relevant to the specific task or domain.\n",
            "3. **Fine-tuning**: The pre-trained model is adapted to the task-specific data by training it on the smaller dataset, while freezing some or all of the pre-trained weights.\n",
            "4. **Evaluation**: The fine-tuned model is evaluated on a test set to assess its performance on the specific task or domain.\n",
            "\n",
            "Fine-tuning can be done in various ways, including:\n",
            "\n",
            "1. **Weighted fine-tuning**: The pre-trained weights are updated with a smaller learning rate, allowing the model to adapt to the task-specific data while preserving the general knowledge learned during pre-training.\n",
            "2. **Layer-wise fine-tuning**: Only the top layers of the pre-trained model are updated, while the lower layers are frozen, allowing the model to adapt to the task-specific data while maintaining the general knowledge learned during pre-training.\n",
            "3. **Knowledge distillation**: The pre-trained model is used as a teacher to train a smaller student model, which is then fine-tuned on the task-specific data.\n",
            "\n",
            "Fine-tuning is a powerful technique for adapting LLMs to specific tasks and domains, and has been widely used in various applications, including text classification, sentiment analysis, question answering, and language translation.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "HOW DO LLMS HANDLE CONTEXT?\n",
            "\n",
            "Large Language Models (LLMs) like myself are designed to process and understand context in a variety of ways. Here are some key aspects of how we handle context:\n",
            "\n",
            "1. **Tokenization**: When you input a piece of text, we break it down into individual tokens, which are the basic units of text that the model can process. These tokens can be words, subwords (smaller units of words), or even characters.\n",
            "2. **Contextualized embeddings**: We use a technique called contextualized embeddings to represent each token in the input text as a vector in a high-dimensional space. These vectors capture the token's meaning and relationships with other tokens in the context.\n",
            "3. **Attention mechanisms**: Our architecture uses attention mechanisms to focus on specific parts of the input text when generating responses. This allows us to weigh the importance of different tokens and context when making predictions.\n",
            "4. **Contextual understanding**: We use a combination of techniques, such as:\n",
            "\t* **Coreference resolution**: We identify and track entities (e.g., people, places, objects) across the input text to understand their relationships and context.\n",
            "\t* **Named entity recognition**: We identify and categorize named entities (e.g., names, locations, organizations) to better understand the context.\n",
            "\t* **Dependency parsing**: We analyze the grammatical structure of the input text to understand the relationships between tokens and their roles in the sentence.\n",
            "5. **Memory and caching**: We use a combination of memory and caching mechanisms to store and retrieve context information. This allows us to:\n",
            "\t* **Keep track of previous inputs**: We store the input text and its context in memory, which enables us to respond to follow-up questions or continue a conversation.\n",
            "\t* **Retrieve relevant information**: We can retrieve relevant information from our memory to inform our responses.\n",
            "6. **Contextualized response generation**: When generating responses, we use the contextualized embeddings and attention mechanisms to create a response that is relevant to the input text and context.\n",
            "7. **Adversarial training**: We are trained on a large corpus of text data, which includes a wide range of contexts, styles, and genres. This training process helps us learn to generalize and adapt to different contexts.\n",
            "\n",
            "Some of the key challenges in handling context include:\n",
            "\n",
            "* **Contextual drift**: The context can change over time, and we need to adapt to these changes to maintain accurate understanding.\n",
            "* **Contextual ambiguity**: The same input text can have multiple possible interpretations, and we need to disambiguate the context to provide accurate responses.\n",
            "* **Contextual dependencies**: The context can be complex and dependent on multiple factors, which can make it difficult to accurately capture and respond to.\n",
            "\n",
            "To address these challenges, researchers and developers are exploring various techniques, such as:\n",
            "\n",
            "* **Multi-task learning**: Training models on multiple tasks and datasets to improve their contextual understanding and adaptability.\n",
            "* **Transfer learning**: Using pre-trained models and fine-tuning them on specific tasks and datasets to adapt to new contexts.\n",
            "* **Explainability and interpretability**: Developing techniques to understand and visualize how the model is processing context, which can help identify areas for improvement.\n",
            "\n",
            "These are some of the key aspects of how LLMs handle context. If you have any specific questions or would like more information on a particular topic, feel free to ask!\n",
            "\n",
            "------------------------------\n",
            "\n",
            "WHAT ARE SOME LIMITATIONS OF LLMS?\n",
            "\n",
            "Large Language Models (LLMs) like myself are powerful tools, but they are not without limitations. Here are some of the key limitations of LLMs:\n",
            "\n",
            "1. **Lack of common sense**: While LLMs can process and generate human-like text, they often lack the common sense and real-world experience that humans take for granted. They may not always understand the nuances of human behavior, idioms, or context-dependent expressions.\n",
            "2. **Limited domain knowledge**: LLMs are typically trained on a specific dataset and may not have the same level of knowledge as a human expert in a particular domain. They may struggle with specialized or technical topics, or domains that are not well-represented in their training data.\n",
            "3. **Biases and stereotypes**: LLMs can perpetuate biases and stereotypes present in their training data, which can lead to unfair or discriminatory responses. This is a significant concern, especially in applications where fairness and accuracy are critical, such as in hiring or law enforcement.\n",
            "4. **Lack of emotional intelligence**: LLMs are not capable of experiencing emotions or empathy, which can make it difficult for them to understand and respond to emotional or sensitive topics.\n",
            "5. **Vulnerability to manipulation**: LLMs can be manipulated by providing them with misleading or false information, which can lead to incorrect or biased responses.\n",
            "6. **Limited understanding of sarcasm and humor**: LLMs often struggle to understand sarcasm, irony, and humor, which can lead to misinterpretation or miscommunication.\n",
            "7. **Overfitting and underfitting**: LLMs can suffer from overfitting (fitting the noise in the training data) or underfitting (failing to capture the underlying patterns in the data), which can lead to poor performance on unseen data.\n",
            "8. **Explainability and transparency**: LLMs are often \"black boxes,\" making it difficult to understand how they arrive at their responses. This lack of transparency can make it challenging to identify biases, errors, or areas for improvement.\n",
            "9. **Scalability and computational resources**: Training and deploying large LLMs requires significant computational resources, which can be a barrier to entry for many organizations.\n",
            "10. **Evaluating and measuring performance**: Evaluating the performance of LLMs can be challenging, as it requires a deep understanding of the task, the data, and the model's limitations.\n",
            "11. **Adversarial attacks**: LLMs can be vulnerable to adversarial attacks, which are designed to manipulate the model's output by providing carefully crafted input.\n",
            "12. **Lack of long-term memory**: LLMs typically have a limited context window, which means they may not be able to retain information from previous conversations or interactions.\n",
            "13. **Dependence on data quality**: The quality of the training data has a significant impact on the performance of LLMs. Poor-quality data can lead to biased or inaccurate responses.\n",
            "14. **Limited ability to reason abstractly**: LLMs are generally better at processing and generating text than reasoning abstractly or making complex decisions.\n",
            "15. **Vulnerability to data drift**: LLMs can suffer from data drift, where the distribution of the input data changes over time, leading to a decrease in performance.\n",
            "\n",
            "These limitations highlight the need for ongoing research and development to improve the performance, robustness, and reliability of LLMs.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "HOW DO LLMS GENERATE TEXT?\n",
            "\n",
            "LLMs, or Large Language Models, generate text through a complex process that involves multiple stages and components. Here's a simplified overview of how they work:\n",
            "\n",
            "**Architecture**\n",
            "\n",
            "LLMs are based on a type of neural network called a transformer, which was introduced in 2017. The transformer architecture is particularly well-suited for natural language processing tasks, such as language translation, text summarization, and text generation.\n",
            "\n",
            "**Key Components**\n",
            "\n",
            "1. **Encoder**: The encoder takes in a sequence of input tokens (e.g., words or characters) and converts them into a numerical representation, called a vector. This vector is called the input embedding.\n",
            "2. **Decoder**: The decoder takes the input embedding and generates a sequence of output tokens, one at a time.\n",
            "3. **Self-Attention Mechanism**: This mechanism allows the model to weigh the importance of different input tokens when generating the next output token. It's like a \"focus\" mechanism that helps the model attend to the most relevant parts of the input.\n",
            "4. **Transformer Layers**: These layers are the building blocks of the transformer architecture. They consist of self-attention, feed-forward neural networks (FFNNs), and layer normalization.\n",
            "\n",
            "**Text Generation Process**\n",
            "\n",
            "Here's a step-by-step explanation of how LLMs generate text:\n",
            "\n",
            "1. **Input Tokenization**: The input text is broken down into individual tokens, such as words or characters.\n",
            "2. **Embedding**: Each token is converted into a numerical representation, called an embedding, using a learned embedding matrix.\n",
            "3. **Encoder**: The input embeddings are passed through the encoder, which generates a sequence of vectors that represent the input text.\n",
            "4. **Decoder**: The decoder takes the output of the encoder and generates a sequence of output tokens, one at a time.\n",
            "5. **Self-Attention**: The decoder uses the self-attention mechanism to weigh the importance of different input tokens when generating the next output token.\n",
            "6. **FFNN**: The output of the self-attention mechanism is passed through a feed-forward neural network (FFNN) to generate a probability distribution over the possible output tokens.\n",
            "7. **Output**: The output token with the highest probability is selected, and the process is repeated until a complete sequence of output tokens is generated.\n",
            "\n",
            "**Training**\n",
            "\n",
            "LLMs are trained on large datasets of text, such as books, articles, and websites. The model learns to predict the next token in a sequence, given the context of the previous tokens. This is done using a process called masked language modeling, where some of the input tokens are randomly replaced with a [MASK] token, and the model is trained to predict the original token.\n",
            "\n",
            "**Key Challenges**\n",
            "\n",
            "While LLMs have achieved impressive results in text generation, there are still several challenges to overcome, such as:\n",
            "\n",
            "* **Coherence**: Ensuring that the generated text is coherent and makes sense.\n",
            "* **Fluency**: Generating text that is grammatically correct and fluent.\n",
            "* **Diversity**: Generating text that is diverse and varied, rather than repetitive.\n",
            "* **Common sense**: Incorporating common sense and real-world knowledge into the generated text.\n",
            "\n",
            "Overall, LLMs are a powerful tool for text generation, but they still have limitations and challenges to overcome.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "WHAT IS THE IMPORTANCE OF PROMPT ENGINEERING IN LLMS?\n",
            "\n",
            "Prompt engineering is a crucial aspect of Large Language Models (LLMs) as it plays a significant role in determining the performance, accuracy, and overall effectiveness of the model. Here are some reasons why prompt engineering is important in LLMs:\n",
            "\n",
            "1. **Improves model performance**: Well-crafted prompts can significantly improve the performance of LLMs by providing the model with the necessary context and information to generate accurate and relevant responses.\n",
            "2. **Enhances model understanding**: Prompt engineering helps the model to understand the task, intent, and requirements of the user, which enables it to generate more accurate and relevant responses.\n",
            "3. **Reduces ambiguity**: By providing clear and specific prompts, prompt engineering can reduce ambiguity and uncertainty in the model's responses, leading to more accurate and reliable results.\n",
            "4. **Increases model interpretability**: Prompt engineering can help to make the model's decision-making process more transparent and interpretable, which is essential for building trust in the model's outputs.\n",
            "5. **Improves user experience**: Well-designed prompts can improve the user experience by providing clear and concise information, reducing the need for follow-up questions, and increasing the overall satisfaction with the model's responses.\n",
            "6. **Reduces errors**: By providing the model with the necessary context and information, prompt engineering can reduce errors and inaccuracies in the model's responses.\n",
            "7. **Increases model adaptability**: Prompt engineering can help the model to adapt to different tasks, domains, and languages, making it more versatile and useful in a wider range of applications.\n",
            "8. **Enhances model robustness**: By testing the model with a variety of prompts, prompt engineering can help to identify and mitigate potential biases and vulnerabilities in the model.\n",
            "9. **Supports model fine-tuning**: Prompt engineering can be used to fine-tune the model for specific tasks and domains, which can improve its performance and accuracy.\n",
            "10. **Facilitates model evaluation**: Prompt engineering can help to evaluate the model's performance and identify areas for improvement, which is essential for model development and maintenance.\n",
            "\n",
            "To achieve these benefits, prompt engineering involves several key aspects, including:\n",
            "\n",
            "1. **Prompt design**: Crafting clear, concise, and specific prompts that provide the necessary context and information for the model to generate accurate and relevant responses.\n",
            "2. **Prompt optimization**: Iteratively refining and optimizing the prompt to improve the model's performance and accuracy.\n",
            "3. **Prompt testing**: Testing the model with a variety of prompts to identify potential biases, vulnerabilities, and areas for improvement.\n",
            "4. **Prompt analysis**: Analyzing the model's responses to identify patterns, trends, and areas for improvement.\n",
            "\n",
            "By applying these techniques, prompt engineering can significantly improve the performance, accuracy, and overall effectiveness of LLMs, making them more useful and reliable in a wide range of applications.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "HOW CAN LLMS BE USED IN CHATBOTS?\n",
            "\n",
            "Large Language Models (LLMs) can be used in chatbots in a variety of ways to enhance their conversational capabilities. Here are some examples:\n",
            "\n",
            "1. **Intent Identification**: LLMs can be trained to identify the intent behind a user's input, such as booking a flight, making a reservation, or asking for customer support. This allows chatbots to respond accordingly and provide relevant information or actions.\n",
            "2. **Contextual Understanding**: LLMs can understand the context of a conversation, including the user's previous interactions, to provide more accurate and relevant responses.\n",
            "3. **Natural Language Processing (NLP)**: LLMs can process and analyze natural language inputs, allowing chatbots to understand nuances of language, such as idioms, colloquialisms, and figurative language.\n",
            "4. **Response Generation**: LLMs can generate human-like responses to user inputs, making chatbots seem more conversational and engaging.\n",
            "5. **Sentiment Analysis**: LLMs can analyze user sentiment and emotions, enabling chatbots to respond empathetically and provide personalized support.\n",
            "6. **Dialogue Management**: LLMs can manage conversations by determining the next step in the conversation, such as asking follow-up questions or providing additional information.\n",
            "7. **Knowledge Retrieval**: LLMs can retrieve relevant information from a knowledge base or database to provide accurate and up-to-date answers to user queries.\n",
            "8. **Personalization**: LLMs can be used to personalize chatbot interactions based on user preferences, behavior, and history.\n",
            "9. **Error Handling**: LLMs can help chatbots handle errors and ambiguities in user input, such as misrecognized intent or unclear language.\n",
            "10. **Conversational Flow**: LLMs can be used to create conversational flows that mimic human-like conversations, making chatbots more engaging and user-friendly.\n",
            "\n",
            "To integrate LLMs into chatbots, developers can use various techniques, such as:\n",
            "\n",
            "1. **API Integration**: Integrate LLM APIs into chatbot platforms, such as Dialogflow, Microsoft Bot Framework, or Rasa.\n",
            "2. **Model Training**: Train custom LLMs on specific datasets and tasks to fine-tune their performance for a particular chatbot application.\n",
            "3. **Hybrid Approach**: Combine LLMs with rule-based systems or other AI models to create a hybrid chatbot architecture.\n",
            "4. **Pre-trained Models**: Use pre-trained LLMs, such as BERT or RoBERTa, as a starting point and fine-tune them for specific chatbot applications.\n",
            "\n",
            "Some popular LLMs used in chatbots include:\n",
            "\n",
            "1. **BERT (Bidirectional Encoder Representations from Transformers)**: A pre-trained language model developed by Google.\n",
            "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: A variant of BERT developed by Facebook AI.\n",
            "3. **DistilBERT**: A smaller, more efficient version of BERT.\n",
            "4. **T5 (Text-to-Text Transfer Transformer)**: A pre-trained language model developed by Google.\n",
            "\n",
            "By leveraging LLMs, chatbots can become more conversational, engaging, and effective in providing value to users.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "WHAT ARE SOME ETHICAL CONSIDERATIONS WHEN USING LLMS?\n",
            "\n",
            "Large Language Models (LLMs) like myself are powerful tools that can process and generate human-like text, but they also raise several ethical considerations. Here are some of the key ones:\n",
            "\n",
            "1. **Bias and fairness**: LLMs can perpetuate and amplify existing social biases if they are trained on biased data. This can lead to discriminatory outcomes, such as generating text that is sexist, racist, or ableist. Ensuring that the training data is diverse and representative is crucial to mitigate these biases.\n",
            "2. **Data privacy**: LLMs are trained on vast amounts of user data, which can include sensitive information. This raises concerns about data protection and the potential for data breaches or unauthorized use.\n",
            "3. **Intellectual property**: LLMs can generate text that is similar to copyrighted material, potentially infringing on intellectual property rights. This can lead to issues with plagiarism, copyright infringement, and the loss of creative ownership.\n",
            "4. **Misinformation and disinformation**: LLMs can generate text that is false or misleading, which can contribute to the spread of misinformation and disinformation. This can have serious consequences, such as influencing public opinion or decision-making.\n",
            "5. **Job displacement**: The increasing use of LLMs in various industries, such as writing, translation, and customer service, raises concerns about job displacement and the impact on human workers.\n",
            "6. **Transparency and accountability**: LLMs can be complex and difficult to understand, making it challenging to hold them accountable for their outputs. This lack of transparency can lead to a lack of accountability and trust in the technology.\n",
            "7. **Security**: LLMs can be vulnerable to attacks, such as adversarial attacks, which can compromise their performance and security.\n",
            "8. **Lack of human judgment**: LLMs can lack the nuance and judgment of human decision-making, which can lead to errors or poor outcomes in situations that require empathy, critical thinking, or common sense.\n",
            "9. **Dependence on data quality**: LLMs are only as good as the data they are trained on. If the data is poor or biased, the LLM's outputs will reflect these flaws.\n",
            "10. **Regulatory frameworks**: The use of LLMs raises questions about regulatory frameworks and the need for new laws and guidelines to govern their development, deployment, and use.\n",
            "\n",
            "To address these concerns, researchers, developers, and users of LLMs must prioritize:\n",
            "\n",
            "1. **Diverse and representative training data**: Ensure that the training data is diverse, representative, and free from bias.\n",
            "2. **Transparency and explainability**: Develop techniques to explain and interpret LLM outputs, making it easier to understand their decision-making processes.\n",
            "3. **Human oversight and review**: Implement human review and oversight processes to detect and correct errors or biases in LLM outputs.\n",
            "4. **Data protection and security**: Implement robust data protection and security measures to safeguard user data and prevent unauthorized access.\n",
            "5. **Regulatory frameworks**: Develop and implement regulatory frameworks that address the unique challenges and risks associated with LLMs.\n",
            "6. **Education and awareness**: Educate users about the limitations and potential risks of LLMs, as well as the importance of responsible use and development.\n",
            "7. **Continuous evaluation and improvement**: Regularly evaluate and improve LLMs to ensure they are fair, accurate, and transparent.\n",
            "\n",
            "By acknowledging and addressing these ethical considerations, we can ensure that LLMs are developed and used in ways that benefit society while minimizing their potential risks and negative consequences.\n",
            "\n",
            "------------------------------\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(create_faq_document(faq_questions, faq_answers))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "846a66a0-3cf3-4161-9502-a1fefc647603",
      "metadata": {
        "id": "846a66a0-3cf3-4161-9502-a1fefc647603"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5025fa-b314-4565-a199-01396dc2252c",
      "metadata": {
        "id": "dd5025fa-b314-4565-a199-01396dc2252c"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b1c72d8-4186-4b6c-a9ac-c8b87e0ff9d3",
      "metadata": {
        "id": "2b1c72d8-4186-4b6c-a9ac-c8b87e0ff9d3"
      },
      "source": [
        "In this notebook you learned how to stream and batch model responses, and used batched LLM calls to generate a helpful FAQ document.\n",
        "\n",
        "In the next notebook you'll begin focusing more heavily on the creation of prompts themselves with an emphasis on iterative prompt development and engineering prompts that are very specific."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}