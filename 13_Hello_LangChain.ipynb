{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ng13/AI_wrkshp/blob/main/13_Hello_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d7daf7d-c0ab-4bf3-b93b-8f0ee88ec52c",
      "metadata": {
        "id": "2d7daf7d-c0ab-4bf3-b93b-8f0ee88ec52c"
      },
      "source": [
        "![nvidia](images/nvidia.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66d5adbf-f5d6-4792-8c5e-64a73b2d3a3d",
      "metadata": {
        "id": "66d5adbf-f5d6-4792-8c5e-64a73b2d3a3d"
      },
      "source": [
        "# Hello World with LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28013522",
      "metadata": {
        "id": "28013522"
      },
      "source": [
        "In this notebook, we will learn how to interact with LangChain to generate chat completions using the Llama 3.1 8b instruct model. This introductory exercise will help you understand the basics of setting up and using LangChain in a Jupyter environment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5a70fb-0429-4036-82ce-a55c4262561a",
      "metadata": {
        "id": "ea5a70fb-0429-4036-82ce-a55c4262561a"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08054f2",
      "metadata": {
        "id": "c08054f2"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a023bc7a-47b5-4508-957c-f3354c9fb363",
      "metadata": {
        "id": "a023bc7a-47b5-4508-957c-f3354c9fb363"
      },
      "source": [
        "By the time you complete this notebook, you will:\n",
        "\n",
        "- Have an introductory understanding of LangChain.\n",
        "- Generate simple chat completions using LangChain.\n",
        "- Compare the differences between using LangChain and the OpenAI library for chat completion."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "500b0fab-b9e3-4de9-bc46-5f31ab9ea623",
      "metadata": {
        "id": "500b0fab-b9e3-4de9-bc46-5f31ab9ea623"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vg89g1zbTPoY",
        "outputId": "0fd5c1ca-18bc-41e3-e8d8-c13f5f4f8c65"
      },
      "id": "Vg89g1zbTPoY",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.76)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.28)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.9)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.43)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (25.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.8.3)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.10.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core langchain-nvidia-ai-endpoints"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZSRGkaTTV7k",
        "outputId": "55d36e1b-ac4e-44dc-f1d0-dac049bff314"
      },
      "id": "DZSRGkaTTV7k",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.76)\n",
            "Collecting langchain-nvidia-ai-endpoints\n",
            "  Downloading langchain_nvidia_ai_endpoints-0.3.18-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.11.9)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from langchain-nvidia-ai-endpoints) (3.12.15)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-nvidia-ai-endpoints)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.20.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (1.3.1)\n",
            "Downloading langchain_nvidia_ai_endpoints-0.3.18-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-nvidia-ai-endpoints\n",
            "Successfully installed filetype-1.2.0 langchain-nvidia-ai-endpoints-0.3.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "327550d4",
      "metadata": {
        "id": "327550d4"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9128a04-4ba5-4762-a277-3e614725214b",
      "metadata": {
        "id": "f9128a04-4ba5-4762-a277-3e614725214b"
      },
      "source": [
        "Here we import the `ChatNVIDIA` class from `langchain_nvidia_ai_endpoints`, which will enable us to interact with our local Llama 3.1 8b instruct NIM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75febe51",
      "metadata": {
        "id": "75febe51"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a291cd0-5701-41dc-b3a4-229bce728f10",
      "metadata": {
        "id": "9a291cd0-5701-41dc-b3a4-229bce728f10"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "669b4723-e7c3-4cb7-bc33-efe73d10d662",
      "metadata": {
        "id": "669b4723-e7c3-4cb7-bc33-efe73d10d662"
      },
      "source": [
        "## Using langchain_nvidia_ai_endpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee597c1a-500b-4eea-8b89-a359988fa534",
      "metadata": {
        "id": "ee597c1a-500b-4eea-8b89-a359988fa534"
      },
      "source": [
        "As you have observed from the last notebook, using OpenAI completions can lead to a lot of code repetition.\n",
        "\n",
        "There has been a lot of effort from developers to utilize AI applications efficiently. Amongst them, [LangChain](https://python.langchain.com/v0.2/docs/introduction/) is a popular LLM orchestration framework that aids users to interact with LLMs easily.\n",
        "\n",
        "LangChain’s simplistic architecture and abstractions let developers effortlessly replace components like language models, prompt, and processing steps, with little modification. In addition, LangChain provides a consistent, unified interface across multiple LLMs from different providers, simplifying interactions and allowing developers to concentrate on application development rather than dealing with model-specific complexities.\n",
        "\n",
        "This library is highly popular and evolves quickly with advancements in the field. While there are many parts of LangChain such as LangGraph, LangSmith, and LangServe, we are going to focus on LangChain core in our workshop today.\n",
        "\n",
        "In order to use LangChain with our locally-hosted model, we need to utilize a Framework Connector which ultimately converts an arbitrary API from its core into one that a target code-base would expect. We can do this by utilizing `ChatNVIDIA` class within `langchain-nvidia-ai-endpoints` package. With this tool, which uses the OpenAI API under the hood, we can start to iteratively develop and test out prompts more efficiently, and use LangChain with our NVIDIA NIM LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "389a972e-fce8-4af5-9a7e-ed940a4fa800",
      "metadata": {
        "id": "389a972e-fce8-4af5-9a7e-ed940a4fa800"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2e2f950-1450-4f55-a4b3-ed2fbc987513",
      "metadata": {
        "id": "b2e2f950-1450-4f55-a4b3-ed2fbc987513"
      },
      "source": [
        "## Setting Up a Model Instance With LangChain"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb09eea9-c0d0-4962-a7d3-1d3f51b8573b",
      "metadata": {
        "id": "cb09eea9-c0d0-4962-a7d3-1d3f51b8573b"
      },
      "source": [
        "To start using LangChain, we need to set up the ChatNVIDIA model instance. This involves configuring the base URL and model name, much as we did in the previous notebook with the `OpenAI` library."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24eb1c99-a4fe-46b6-9900-f77c1db9f1e5",
      "metadata": {
        "id": "24eb1c99-a4fe-46b6-9900-f77c1db9f1e5"
      },
      "outputs": [],
      "source": [
        "llm = ChatNVIDIA(\n",
        "  model=\"meta/llama-3.1-8b-instruct\",\n",
        "  api_key=\"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\",\n",
        "  temperature=0.2,\n",
        "  top_p=0.7,\n",
        "  max_completion_tokens=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90dd717d-ee8e-4607-b9c5-11bb39ea6bb7",
      "metadata": {
        "id": "90dd717d-ee8e-4607-b9c5-11bb39ea6bb7"
      },
      "source": [
        "You may have noticed we set a value called `temperature` to `0`. `temperature`. which is a floating point value between `0` and `1` is a way to control the randomness of a model's responses. When set to `0`, the LLM will always generate the text that it considers as having the highest probability of coming next. When set to higher values, it can generate text that is not necessarily what it considers to be the highest probability of coming next therefore introducing randomness and a sense of creativity in its generations.\n",
        "\n",
        "We won't discuss modifying `temperature` to higher values in great detail, but remember, set it to `0` if you want deterministic responses, and set it higher if you want less deterministic (i.e. more creative) responses.\n",
        "\n",
        "For those of you interested in learning more about how `temperature` and some other additional hyperparameters work, feel free to check out the appendix notebook, located in this directory, [99-Appendix-Hyperparams](99-Appendix-Hyperparams.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f20cacd2-3024-4880-ac02-99ae957d9c2d",
      "metadata": {
        "id": "f20cacd2-3024-4880-ac02-99ae957d9c2d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4443786e-e1f6-4f7e-92ae-ca49cf18a571",
      "metadata": {
        "id": "4443786e-e1f6-4f7e-92ae-ca49cf18a571"
      },
      "source": [
        "## Making a Simple Request"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ba14bb4-407b-4060-b967-b417813ca022",
      "metadata": {
        "id": "1ba14bb4-407b-4060-b967-b417813ca022"
      },
      "source": [
        "We can now start sending chat completion prompts to our model. We'll begin by using the `invoke` method, which we hope you'll agree is much easier to use than was the OpenAI client in the previous notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e779016-7071-4b44-aef0-72da438ebba0",
      "metadata": {
        "id": "4e779016-7071-4b44-aef0-72da438ebba0"
      },
      "outputs": [],
      "source": [
        "prompt = 'Who are you?'\n",
        "result = llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "607b5258-a3af-4f69-92d9-0954bfb9f591",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "607b5258-a3af-4f69-92d9-0954bfb9f591",
        "outputId": "010f5817-fa54-4841-afde-3698d69181f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"' additional_kwargs={} response_metadata={'role': 'assistant', 'content': 'I\\'m an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"', 'token_usage': {'prompt_tokens': 14, 'total_tokens': 36, 'completion_tokens': 22}, 'finish_reason': 'stop', 'model_name': 'meta/llama-3.1-8b-instruct'} id='run--7f9ad379-2fb9-4387-b3de-d7ea955d5df4-0' usage_metadata={'input_tokens': 14, 'output_tokens': 22, 'total_tokens': 36} role='assistant'\n"
          ]
        }
      ],
      "source": [
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "092f218c-b0ec-4ec0-ade3-906f3f4edc82",
      "metadata": {
        "id": "092f218c-b0ec-4ec0-ade3-906f3f4edc82"
      },
      "source": [
        "The result is similar to what we obtained using the OpenAI client, but it also includes metadata about the conversation and token usage. This will be useful for maintaining conversation context in more advanced applications.\n",
        "\n",
        "To extract just the response from the model, we can simply use the result's `content` property, as below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d02fcfa-0e81-46ca-9480-61fb0ce0e764",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d02fcfa-0e81-46ca-9480-61fb0ce0e764",
        "outputId": "94a5e297-275b-4200-a23f-80869a0a9761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I'm an artificial intelligence model known as Llama. Llama stands for \"Large Language Model Meta AI.\"\n"
          ]
        }
      ],
      "source": [
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2973e10f-0bf4-4fde-851c-410fc960544a",
      "metadata": {
        "id": "2973e10f-0bf4-4fde-851c-410fc960544a"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2839a2d-65e3-41b3-afd9-ee0982441d67",
      "metadata": {
        "id": "f2839a2d-65e3-41b3-afd9-ee0982441d67"
      },
      "source": [
        "## Exercise: Generate Your Own Completion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd0d6caf-fa74-4709-bd3b-07b70446a416",
      "metadata": {
        "id": "dd0d6caf-fa74-4709-bd3b-07b70446a416"
      },
      "source": [
        "Use our existing model instance `llm` to generate and print a response from our local Llama 3.1 model to a prompt of your choice."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bd133b8-f6cb-4cd1-b0a1-5c114531fba9",
      "metadata": {
        "id": "4bd133b8-f6cb-4cd1-b0a1-5c114531fba9"
      },
      "source": [
        "### Your Work Here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0142c794-0aa9-4533-a6fe-58bb3ea68f11",
      "metadata": {
        "id": "0142c794-0aa9-4533-a6fe-58bb3ea68f11"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "994857c1-9d1a-4383-8eb8-818daf659768",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "994857c1-9d1a-4383-8eb8-818daf659768"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55d2e2a3-63bd-4b9b-93ac-dbba2830947f",
      "metadata": {
        "id": "55d2e2a3-63bd-4b9b-93ac-dbba2830947f"
      },
      "outputs": [],
      "source": [
        "prompt = 'Give me 3 puns having to do with LangChain.'\n",
        "result = llm.invoke(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fbd103e-86a3-4992-bda4-bf221a0a4fb3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fbd103e-86a3-4992-bda4-bf221a0a4fb3",
        "outputId": "affcd78f-9867-4057-e461-f2bcba908ccf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pun-derful! Here are three LangChain puns for you:\n",
            "\n",
            "1. LangChain is the \"chain\" reaction to AI innovation.\n",
            "2. LangChain is the \"link\" between humans and AI.\n",
            "3. LangChain is the \"chain\" that binds language models together.\n",
            "\n",
            "Hope these puns \"chain\" you up!\n"
          ]
        }
      ],
      "source": [
        "print(result.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840",
      "metadata": {
        "id": "2ee2e7bf-abf5-4ece-88e0-0e1da8cb0840"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd5025fa-b314-4565-a199-01396dc2252c",
      "metadata": {
        "id": "dd5025fa-b314-4565-a199-01396dc2252c"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b1c72d8-4186-4b6c-a9ac-c8b87e0ff9d3",
      "metadata": {
        "id": "2b1c72d8-4186-4b6c-a9ac-c8b87e0ff9d3"
      },
      "source": [
        "By completing this notebook, you should now have a basic understanding of how to use LangChain to generate chat completions and parse out the model response, which we hope you'll agree, is quite straight forward.\n",
        "\n",
        "In the next notebook, you'll go a little further into using chat completions with LangChain by learning how to stream model responses and handle multiple chat completion requests in batches."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}