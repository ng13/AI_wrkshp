{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ng13/AI_wrkshp/blob/main/35_Chatbots.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e977866-17c1-4158-aa61-e2cf61211db6",
      "metadata": {
        "id": "7e977866-17c1-4158-aa61-e2cf61211db6"
      },
      "source": [
        "![NVIDIA](images/nvidia.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "108dd8bb-c86c-40fd-a164-991673b2cc21",
      "metadata": {
        "id": "108dd8bb-c86c-40fd-a164-991673b2cc21"
      },
      "source": [
        "# Chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28013522",
      "metadata": {
        "id": "28013522"
      },
      "source": [
        "In this notebook, you will learn how to store conversation history and thereby enable chatbot functionality in your LLM-based chains."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69366671-11a4-4439-b6ad-cb89497ef5d4",
      "metadata": {
        "id": "69366671-11a4-4439-b6ad-cb89497ef5d4"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c08054f2",
      "metadata": {
        "id": "c08054f2"
      },
      "source": [
        "## Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a023bc7a-47b5-4508-957c-f3354c9fb363",
      "metadata": {
        "id": "a023bc7a-47b5-4508-957c-f3354c9fb363"
      },
      "source": [
        "By the time you complete this notebook you will:\n",
        "\n",
        "- Understand the core principles and techniques required to create chatbot applications capable of retaining conversation history.\n",
        "- Create easy-to-use chatbots, capable of assuming a variety of different personas.\n",
        "- Interact with a simple chatbot application interface, well-suited for chatbot application prototyping."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb",
      "metadata": {
        "id": "f4e9ea06-814f-43fd-9f59-ce67dfcb1bbb"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "327550d4",
      "metadata": {
        "id": "327550d4"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-core langchain-nvidia-ai-endpoints"
      ],
      "metadata": {
        "id": "GOSiIvplZSC6",
        "outputId": "d0dbed7c-b0b0-4089-d789-4c43ca52a148",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "GOSiIvplZSC6",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (0.3.76)\n",
            "Collecting langchain-nvidia-ai-endpoints\n",
            "  Downloading langchain_nvidia_ai_endpoints-0.3.18-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.28)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.11.9)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.9.1 in /usr/local/lib/python3.12/dist-packages (from langchain-nvidia-ai-endpoints) (3.12.15)\n",
            "Collecting filetype<2.0.0,>=1.2.0 (from langchain-nvidia-ai-endpoints)\n",
            "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4.0.0,>=3.9.1->langchain-nvidia-ai-endpoints) (1.20.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (3.11.3)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core) (0.4.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.3.45->langchain-core) (1.3.1)\n",
            "Downloading langchain_nvidia_ai_endpoints-0.3.18-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
            "Installing collected packages: filetype, langchain-nvidia-ai-endpoints\n",
            "Successfully installed filetype-1.2.0 langchain-nvidia-ai-endpoints-0.3.18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75febe51",
      "metadata": {
        "id": "75febe51"
      },
      "outputs": [],
      "source": [
        "from langchain_nvidia_ai_endpoints import ChatNVIDIA\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d",
      "metadata": {
        "id": "9bc7aca5-1c4a-4cb0-bd28-d8a7636cc96d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c0e1244",
      "metadata": {
        "id": "5c0e1244"
      },
      "source": [
        "## Create a Model Instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2c05c82",
      "metadata": {
        "id": "c2c05c82"
      },
      "outputs": [],
      "source": [
        "llm = ChatNVIDIA(\n",
        "  model=\"meta/llama-3.1-8b-instruct\",\n",
        "  api_key=\"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\",\n",
        "  temperature=0.2,\n",
        "  top_p=0.7,\n",
        "  max_completion_tokens=1024,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2a97370-d9b4-41d4-995e-47d2f80e2b73",
      "metadata": {
        "id": "b2a97370-d9b4-41d4-995e-47d2f80e2b73"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f694b45-d57d-49bb-aedb-158a34d49997",
      "metadata": {
        "id": "4f694b45-d57d-49bb-aedb-158a34d49997"
      },
      "source": [
        "## Placeholder Messages"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b86f9a6-c6ed-4e36-98c3-3bf2333574e5",
      "metadata": {
        "id": "7b86f9a6-c6ed-4e36-98c3-3bf2333574e5"
      },
      "source": [
        "Before we begin work enabling conversation history and chatbot functionality, we need to introduce a new kind of message that we have not yet covered, **placeholder** messages.\n",
        "\n",
        "Put simply, and as the name suggest, a placeholder message is used in a prompt template to hold the place of a list of other messages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed769665-d4fd-4987-871a-103c7820cad8",
      "metadata": {
        "id": "ed769665-d4fd-4987-871a-103c7820cad8"
      },
      "outputs": [],
      "source": [
        "template_with_placeholder = ChatPromptTemplate.from_messages([\n",
        "    ('placeholder', '{messages}'),\n",
        "    ('human', '{prompt}')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3c746c1-f1fa-41b7-86b0-0bad147c28b0",
      "metadata": {
        "id": "b3c746c1-f1fa-41b7-86b0-0bad147c28b0"
      },
      "outputs": [],
      "source": [
        "messages = [\n",
        "    ('human', 'The sun came up today.'),\n",
        "    ('ai', 'That is wonderful!'),\n",
        "    ('human', 'The sun went down today.'),\n",
        "    ('ai', 'That is also wonderful!.')\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "261f7cf8-017d-4028-95b5-37e4da2d4887",
      "metadata": {
        "id": "261f7cf8-017d-4028-95b5-37e4da2d4887"
      },
      "outputs": [],
      "source": [
        "prompt = 'What happened today?'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed0b9906-606e-4df8-a4d5-f585ce5f972c",
      "metadata": {
        "id": "ed0b9906-606e-4df8-a4d5-f585ce5f972c"
      },
      "source": [
        "When invoking (or streaming or batching) prompt templates or chains that contain placeholder messages, we provide a value as the template indicates, only in the case of a placeholder message, we provide a list of other messages, rather than a string.\n",
        "\n",
        "Here we invoke `template_with_placeholder`, passing in the `messages` list to fulfil the template's `messages` parameter, and the `prompt` string to fulfil its `prompt` parameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "597606a8-e779-4008-afce-acda2c5c70c0",
      "metadata": {
        "id": "597606a8-e779-4008-afce-acda2c5c70c0",
        "outputId": "916f3abb-ac1f-431b-c5fb-37c07dcc781e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[HumanMessage(content='The sun came up today.'), AIMessage(content='That is wonderful!'), HumanMessage(content='The sun went down today.'), AIMessage(content='That is also wonderful!.'), HumanMessage(content='What happened today?')])"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "template_with_placeholder.invoke({'messages': messages, 'prompt': prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4fca50f-1f15-483c-aa7a-7ced61bb42ac",
      "metadata": {
        "id": "a4fca50f-1f15-483c-aa7a-7ced61bb42ac"
      },
      "source": [
        "As you can see, LangChain expanded the `placeholder` message, for which we provided a list, into a list of individual messages that we provided when invoking the prompt template."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47bf6210-c1b7-48a4-9836-2bfca0cdf4d3",
      "metadata": {
        "id": "47bf6210-c1b7-48a4-9836-2bfca0cdf4d3"
      },
      "source": [
        "It should come as no surprise that we can use this prompt template in chains just as we would any other."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "beaab7c9-4b68-425f-9707-41a5f2fd016b",
      "metadata": {
        "id": "beaab7c9-4b68-425f-9707-41a5f2fd016b"
      },
      "outputs": [],
      "source": [
        "chain = template_with_placeholder | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da24eef3-f18a-468a-a95f-822fc4c59c37",
      "metadata": {
        "id": "da24eef3-f18a-468a-a95f-822fc4c59c37",
        "outputId": "b38ce232-c893-44b8-d684-3459f949c181"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"It sounds like the sun rose and set, which is a normal part of the day-night cycle. That's a pretty ordinary day! Did anything else notable happen today?\""
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chain.invoke({'messages': messages, 'prompt': prompt})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34516e58-a46d-432f-8d9b-20bb78df575f",
      "metadata": {
        "id": "34516e58-a46d-432f-8d9b-20bb78df575f"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fa2541e-bd11-4bbe-892c-cd6b1b4b2059",
      "metadata": {
        "id": "1fa2541e-bd11-4bbe-892c-cd6b1b4b2059"
      },
      "source": [
        "## Rudimentary Conversation History"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84b7e30f-7e29-4d9b-be66-828a79fa4d31",
      "metadata": {
        "id": "84b7e30f-7e29-4d9b-be66-828a79fa4d31"
      },
      "source": [
        "We can easily construct a rudimentary conversation history mechanism using a message placeholder. First we'll create a prompt template utilizing a placeholder, and use it in a simple chain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7983475b-98e6-476d-a41b-b6b17441aac8",
      "metadata": {
        "id": "7983475b-98e6-476d-a41b-b6b17441aac8"
      },
      "outputs": [],
      "source": [
        "chat_conversation_template = ChatPromptTemplate.from_messages([\n",
        "    ('placeholder', '{chat_conversation}')\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65fd62a9-3977-44b7-9e05-748c2a05030b",
      "metadata": {
        "id": "65fd62a9-3977-44b7-9e05-748c2a05030b"
      },
      "outputs": [],
      "source": [
        "chat_chain = chat_conversation_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "103aa6de-cdbd-4644-a2fd-81eb5a5eee01",
      "metadata": {
        "id": "103aa6de-cdbd-4644-a2fd-81eb5a5eee01"
      },
      "source": [
        "Next we'll create a list to store our conversation, which we will add to over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439c6420-813f-4a98-8c0e-4c1c76f42d16",
      "metadata": {
        "id": "439c6420-813f-4a98-8c0e-4c1c76f42d16"
      },
      "outputs": [],
      "source": [
        "chat_conversation = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc246a3e-aad6-4a72-a018-45998cfc79bd",
      "metadata": {
        "id": "fc246a3e-aad6-4a72-a018-45998cfc79bd"
      },
      "source": [
        "We will begin by appending our first prompt, as a `user` message to the `chat_conversation` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c507c3c5-56b7-4660-a4b2-a05b51b45840",
      "metadata": {
        "id": "c507c3c5-56b7-4660-a4b2-a05b51b45840"
      },
      "outputs": [],
      "source": [
        "chat_conversation.append(('user', 'Hello, my name is Michael.'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dcf30b9-44fd-4353-85d3-78d8ddfcedbe",
      "metadata": {
        "id": "6dcf30b9-44fd-4353-85d3-78d8ddfcedbe"
      },
      "source": [
        "Just to test it out, we can now invoke our `chat_chain` with the current `chat_conversation` list."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e7d18de-e74d-4641-bc44-24e43fee4107",
      "metadata": {
        "id": "2e7d18de-e74d-4641-bc44-24e43fee4107",
        "outputId": "1456e418-9438-4270-9e2f-edaad74261b3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"Hello Michael! It's nice to meet you. Is there something I can help you with or would you like to chat?\""
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_chain.invoke({'chat_conversation': chat_conversation})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e69e1c6-4ff4-4100-8aa9-2837da5fc191",
      "metadata": {
        "id": "8e69e1c6-4ff4-4100-8aa9-2837da5fc191"
      },
      "source": [
        "It looks like the LLM is able to respond just fine. Since we are wanting to keep track of the conversation history, however, let's invoke the chain again, but this time append the response to the `chat_conversation` list as an `ai` message."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc2a5f46-30f3-4ad6-a1ba-11a44da0d497",
      "metadata": {
        "id": "bc2a5f46-30f3-4ad6-a1ba-11a44da0d497"
      },
      "outputs": [],
      "source": [
        "response = chat_chain.invoke({'chat_conversation': chat_conversation})\n",
        "chat_conversation.append(('ai', response))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7caacfc9-88c0-4245-9162-eba794a6c3cd",
      "metadata": {
        "id": "7caacfc9-88c0-4245-9162-eba794a6c3cd"
      },
      "source": [
        "Looking at `chat_conversation` we see it now contains a list of the messages thus far."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6a004539-22ad-451f-a117-04082703aeff",
      "metadata": {
        "id": "6a004539-22ad-451f-a117-04082703aeff",
        "outputId": "b05791d4-fd18-4936-b77a-b1bd8d3580fc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('user', 'Hello, my name is Michael.'),\n",
              " ('ai',\n",
              "  \"Hello Michael! It's nice to meet you. Is there something I can help you with or would you like to chat?\")]"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "chat_conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7953de91-b8c6-413c-b26a-eb2c956af601",
      "metadata": {
        "id": "7953de91-b8c6-413c-b26a-eb2c956af601"
      },
      "source": [
        "Let's repeat this same process with a new message, and let's pass in a prompt that relies on previous conversation history to answer correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56199c2f-22ea-42cc-bb9f-4c103df645c8",
      "metadata": {
        "id": "56199c2f-22ea-42cc-bb9f-4c103df645c8"
      },
      "outputs": [],
      "source": [
        "chat_conversation.append(('user', 'Do you remember what my name is?'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9cc2d70-d0d1-4cd6-9236-ca6b2bed4735",
      "metadata": {
        "id": "c9cc2d70-d0d1-4cd6-9236-ca6b2bed4735",
        "outputId": "6d4771d7-4abf-4812-cfa0-7b660db39864"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('user', 'Hello, my name is Michael.'),\n",
              " ('ai',\n",
              "  \"Hello Michael! It's nice to meet you. Is there something I can help you with or would you like to chat?\"),\n",
              " ('user', 'Do you remember what my name is?'),\n",
              " ('ai',\n",
              "  \"Your name is Michael. I'll remember it for our conversation. How's your day going so far?\")]"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response = chat_chain.invoke({'chat_conversation': chat_conversation})\n",
        "chat_conversation.append(('ai', response))\n",
        "chat_conversation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "298ac6c2-da8e-45ef-87c8-a3f5d4001ec5",
      "metadata": {
        "id": "298ac6c2-da8e-45ef-87c8-a3f5d4001ec5"
      },
      "source": [
        "As you can see, by appending user prompt and AI responses to `chat_conversation` as `user` and `ai` messages respectively, and then invoking our placeholder-containing `chat_chain` with the entire up-to-date conversation, we now have the ability to converse with the LLM in a way where it retains details from earlier in the conversation.\n",
        "\n",
        "At its most basic level, all chatbot functionality that is capable of retaining conversation history utilizes this method of passing in the conversation prior to new user messages."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef6ebe77-7f65-445b-bde2-2180233ced28",
      "metadata": {
        "id": "ef6ebe77-7f65-445b-bde2-2180233ced28"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a24a85b-6bca-4a36-b7f9-5aa32938a239",
      "metadata": {
        "id": "3a24a85b-6bca-4a36-b7f9-5aa32938a239"
      },
      "source": [
        "## Chatbot Class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a25f0cf-ea6e-4e99-b9da-beb6782b4f0a",
      "metadata": {
        "id": "1a25f0cf-ea6e-4e99-b9da-beb6782b4f0a"
      },
      "source": [
        "We can encapsulate the functionality we acheived above into a class that will make interacting with our conversation history-enabled LLM much simpler. Please read following `Chatbot` class definition, including its comments, carefully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31f5c608-c8ba-4c93-92d2-c55d7f737289",
      "metadata": {
        "id": "31f5c608-c8ba-4c93-92d2-c55d7f737289"
      },
      "outputs": [],
      "source": [
        "class Chatbot:\n",
        "    def __init__(self, llm):\n",
        "        # This is the same prompt template we used earlier, which a placeholder message for storing conversation history.\n",
        "        chat_conversation_template = ChatPromptTemplate.from_messages([\n",
        "            ('placeholder', '{chat_conversation}')\n",
        "        ])\n",
        "\n",
        "        # This is the same chain we created above, added to `self` for use by the `chat` method below.\n",
        "        self.chat_chain = chat_conversation_template | llm | StrOutputParser()\n",
        "\n",
        "        # Here we instantiate an empty list that will be added to over time.\n",
        "        self.chat_conversation = []\n",
        "\n",
        "    # `chat` expects a simple string prompt.\n",
        "    def chat(self, prompt):\n",
        "        # Append the prompt as a user message to chat conversation.\n",
        "        self.chat_conversation.append(('user', prompt))\n",
        "\n",
        "        response = self.chat_chain.invoke({'chat_conversation': self.chat_conversation})\n",
        "        # Append the chain response as an `ai` message to chat conversation.\n",
        "        self.chat_conversation.append(('ai', response))\n",
        "        # Return the chain response to the user for viewing.\n",
        "        return response\n",
        "\n",
        "    # Clear conversation history.\n",
        "    def clear(self):\n",
        "        self.chat_conversation = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c753de7-2db8-4ba0-9459-1afb650edbb9",
      "metadata": {
        "id": "2c753de7-2db8-4ba0-9459-1afb650edbb9"
      },
      "source": [
        "Let's instantiate a chatbot instance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f440020-9798-4138-8293-362355a997ab",
      "metadata": {
        "id": "1f440020-9798-4138-8293-362355a997ab"
      },
      "outputs": [],
      "source": [
        "chatbot = Chatbot(llm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3acefbb0-75c0-44f7-9199-215987e64166",
      "metadata": {
        "id": "3acefbb0-75c0-44f7-9199-215987e64166"
      },
      "source": [
        "We can now utilize its `chat` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28f252c7-0ed7-4160-8f25-2c6a00cf80b7",
      "metadata": {
        "id": "28f252c7-0ed7-4160-8f25-2c6a00cf80b7",
        "outputId": "87e609c4-6a22-4951-cff1-7747f04d2ac0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hi Michael! It's nice to meet you. Is there something I can help you with or would you like to chat?\n"
          ]
        }
      ],
      "source": [
        "print(chatbot.chat('Hi, my name is Michael.'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2bf6b3f-7d5f-4941-96fc-76a6f527669c",
      "metadata": {
        "id": "e2bf6b3f-7d5f-4941-96fc-76a6f527669c",
        "outputId": "5dda450b-02c3-48c5-e51b-8555156384f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Your name is Michael.\n"
          ]
        }
      ],
      "source": [
        "print(chatbot.chat('I just want to be reminded of my name please.'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a9067544-52a7-4b73-bcd4-d82d2f988fe7",
      "metadata": {
        "id": "a9067544-52a7-4b73-bcd4-d82d2f988fe7",
        "outputId": "2b329cc7-2854-40a2-eb7f-891d498a4559"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A math enthusiast, eh? Here's something interesting:\n",
            "\n",
            "Did you know that pi (π) is an irrational number, which means it can't be expressed as a finite decimal or fraction. However, what's even more fascinating is that pi is also a transcendental number, which means it's not the root of any polynomial equation with rational coefficients. In other words, there's no simple formula that can express pi as a finite combination of integers and roots.\n",
            "\n",
            "But here's the really cool part: pi is connected to the Fibonacci sequence! The digits of pi are not randomly distributed, and in fact, the frequency of certain digit combinations in pi is similar to the frequency of those combinations in the Fibonacci sequence. This is known as the \"Fibonacci hypothesis\" or \"Benford's law\" for pi. It's still an open question in mathematics whether this is a coincidence or a deeper mathematical connection.\n",
            "\n",
            "Mind blown, Michael?\n"
          ]
        }
      ],
      "source": [
        "print(chatbot.chat(\"Tell me something interesting I probably don't know about pi.\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19962202-26bf-4a22-9d56-f3a3ff83cf60",
      "metadata": {
        "id": "19962202-26bf-4a22-9d56-f3a3ff83cf60",
        "outputId": "77edb2bc-fef9-496c-bb30-60cb138c7472"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Here's another one:\n",
            "\n",
            "Did you know that pi is connected to the geometry of the universe? The ratio of a circle's circumference to its diameter is pi, and this ratio appears in many natural phenomena, such as:\n",
            "\n",
            "* The shape of galaxies and galaxy clusters\n",
            "* The arrangement of leaves on stems\n",
            "* The branching of trees\n",
            "* The flow of water in rivers and streams\n",
            "* The structure of DNA\n",
            "\n",
            "In fact, pi is a fundamental constant that appears in many areas of mathematics, physics, and engineering, including:\n",
            "\n",
            "* The laws of gravity and motion (Einstein's general relativity)\n",
            "* The behavior of waves and oscillations (acoustics and electromagnetism)\n",
            "* The design of electronic circuits and computer chips\n",
            "* The study of population growth and epidemiology\n",
            "\n",
            "Pi is like a thread that weaves through many different areas of mathematics and science, connecting seemingly unrelated phenomena and patterns.\n",
            "\n",
            "Pretty mind-blowing, right, Michael?\n"
          ]
        }
      ],
      "source": [
        "print(chatbot.chat(\"That's really cool! Give me another.\")) # Note we are not being specific about what \"another\" refers to...the LLM needs to have previous messages to understand our intent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5d5cd17-fe8b-4258-a5bb-420f75dbd464",
      "metadata": {
        "id": "a5d5cd17-fe8b-4258-a5bb-420f75dbd464"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "18fc08b7-b2b2-414b-8b1b-2674fd19a9d5",
      "metadata": {
        "id": "18fc08b7-b2b2-414b-8b1b-2674fd19a9d5"
      },
      "source": [
        "## More Advanced Chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02521de6-3d9d-4924-90f2-8256e0a17769",
      "metadata": {
        "id": "02521de6-3d9d-4924-90f2-8256e0a17769"
      },
      "source": [
        "The topic of managing conversation history, and therefore of creating chatbots is actually quite large and there are many more advanced techniques that are outside the scope of this workshop. We do, however, want to provide you with some additional references for further study, should you wish to pursue the topic further.\n",
        "\n",
        "- [Session-based conversation history, and history trimming](https://python.langchain.com/docs/how_to/chatbots_memory/): LangChain ships with tooling for wrapping chains in a way that provides history management, and specifically, managing multiple conversation sessions. This resource introduces the use of LangChain tooling for managing session-based conversation history, and also covers some techniques for managing the length of conversation history via message trimming and summarization, an important topic as chat conversations may grow large, or even too large to continue passing into an LLM.\n",
        "- [Conversational RAG](https://python.langchain.com/docs/tutorials/qa_chat_history/): Retrieval Augmented Generation, or RAG (see [this DLI self-paced course](https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-15+V1) for more) is a technique whereby LLMs can be provided real-time context from external data sources in support generating thier response. This resource discusses RAG in the context of chatbots capable of retaining conversation history."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cd2d05a-2c82-4760-8aed-87bc9b20c139",
      "metadata": {
        "id": "0cd2d05a-2c82-4760-8aed-87bc9b20c139"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6a4acec2-610d-4c73-9adb-35644d66d02c",
      "metadata": {
        "id": "6a4acec2-610d-4c73-9adb-35644d66d02c"
      },
      "source": [
        "## Exercise: Enable Role-based Chatbots"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ec8290a-e83a-4f8f-a593-39f8c6cc708d",
      "metadata": {
        "id": "1ec8290a-e83a-4f8f-a593-39f8c6cc708d"
      },
      "source": [
        "For this exercise you'll enable your chatbot instances to assume a specific role by leveraging a system message.\n",
        "\n",
        "Below is the class definition for `ChatbotWithRole` which currently is identical to the `Chatbot` class definition above except that we've defined a `system_message` argument (defaulting to an empty string) that be used when instantiating `ChatbotWithRole` instances.\n",
        "\n",
        "Edit the class definition as needed to that you can supply a system message that will create a specific role for your chatbot. Upon completion you should be able to use system messages like the following to create an overarching role for your chatbot to assume.\n",
        "\n",
        "Feel free to check out the *Solution* below if you get stuck."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2c7da8f-4ae5-4b03-acf0-e0fa189fc8af",
      "metadata": {
        "id": "b2c7da8f-4ae5-4b03-acf0-e0fa189fc8af"
      },
      "outputs": [],
      "source": [
        "brief_chatbot_system_message = \"You always answer as briefly and concisely as possible.\"\n",
        "\n",
        "curious_chatbot_system_message = \"\"\"\\\n",
        "You are incredibly curious, and often respond with reflections and followup questions that lean the conversation in the direction of playfully \\\n",
        "understanding more about the subject matters of the conversation.\"\"\"\n",
        "\n",
        "increased_vocabulary_system_message = \"\"\"\\\n",
        "You always respond using challenging and often under-utilized vocabulary words, even when your response could be made more simply.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a3e17f9-87a2-47e3-8870-22ae908713dc",
      "metadata": {
        "id": "8a3e17f9-87a2-47e3-8870-22ae908713dc"
      },
      "source": [
        "### Your Work Here"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8607e5fa-e4a3-4b4c-aca7-c733d162970f",
      "metadata": {
        "id": "8607e5fa-e4a3-4b4c-aca7-c733d162970f"
      },
      "source": [
        "Update the following class definition so that the passed-in `system_message` is effectively utilized by chatbot instances."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a20c14-9050-47ed-a965-f31d045d3281",
      "metadata": {
        "id": "d5a20c14-9050-47ed-a965-f31d045d3281"
      },
      "outputs": [],
      "source": [
        "class ChatbotWithRole:\n",
        "    def __init__(self, llm, system_message=''):\n",
        "        # This is the same prompt template we used earlier, which a placeholder message for storing conversation history.\n",
        "        chat_conversation_template = ChatPromptTemplate.from_messages([\n",
        "            ('placeholder', '{chat_conversation}')\n",
        "        ])\n",
        "\n",
        "        # This is the same chain we created above, added to `self` for use by the `chat` method below.\n",
        "        self.chat_chain = chat_conversation_template | llm | StrOutputParser()\n",
        "\n",
        "        # Here we instantiate an empty list that will be added to over time.\n",
        "        self.chat_conversation = []\n",
        "\n",
        "    # `chat` expects a simple string prompt.\n",
        "    def chat(self, prompt):\n",
        "        # Append the prompt as a user message to chat conversation.\n",
        "        self.chat_conversation.append(('user', prompt))\n",
        "\n",
        "        response = self.chat_chain.invoke({'chat_conversation': self.chat_conversation})\n",
        "        # Append the chain response as an `ai` message to chat conversation.\n",
        "        self.chat_conversation.append(('ai', response))\n",
        "        # Return the chain response to the user for viewing.\n",
        "        return response\n",
        "\n",
        "    # Clear conversation history.\n",
        "    def clear(self):\n",
        "        self.chat_conversation = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd17f1d0-0beb-4504-ab29-dbbab4dc80e8",
      "metadata": {
        "id": "cd17f1d0-0beb-4504-ab29-dbbab4dc80e8"
      },
      "source": [
        "### Try Out a Chatbot With a Role"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74ee613b-5a2a-4deb-9090-fa329d681692",
      "metadata": {
        "id": "74ee613b-5a2a-4deb-9090-fa329d681692"
      },
      "source": [
        "After successfully implementing `ChatbotWithRole`, try creating an instance of it with a system message of your choosing and interact with it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1773a85-5502-4282-b2f6-303b131073e2",
      "metadata": {
        "id": "d1773a85-5502-4282-b2f6-303b131073e2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "be727103-a6d3-4f7b-a02d-744832507d4c",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "be727103-a6d3-4f7b-a02d-744832507d4c"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3b61c08-fc3a-4b96-9bce-99df71a22cd1",
      "metadata": {
        "id": "e3b61c08-fc3a-4b96-9bce-99df71a22cd1"
      },
      "source": [
        "The solution is brief. Here we added an additional system message to the `chat_conversation_template` that uses the passed-in `system_message`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca74fdd6-00e8-4bc5-8b84-ccddcdae3fa7",
      "metadata": {
        "id": "ca74fdd6-00e8-4bc5-8b84-ccddcdae3fa7"
      },
      "outputs": [],
      "source": [
        "class ChatbotWithRole:\n",
        "    def __init__(self, llm, system_message=''):\n",
        "        # This is the same prompt template we used earlier, which a placeholder message for storing conversation history.\n",
        "        chat_conversation_template = ChatPromptTemplate.from_messages([\n",
        "            ('system', system_message),\n",
        "            ('placeholder', '{chat_conversation}')\n",
        "        ])\n",
        "\n",
        "        # This is the same chain we created above, added to `self` for use by the `chat` method below.\n",
        "        self.chat_chain = chat_conversation_template | llm | StrOutputParser()\n",
        "\n",
        "        # Here we instantiate an empty list that will be added to over time.\n",
        "        self.chat_conversation = []\n",
        "\n",
        "    # `chat` expects a simple string prompt.\n",
        "    def chat(self, prompt):\n",
        "        # Append the prompt as a user message to chat conversation.\n",
        "        self.chat_conversation.append(('user', prompt))\n",
        "\n",
        "        response = self.chat_chain.invoke({'chat_conversation': self.chat_conversation})\n",
        "        # Append the chain response as an `ai` message to chat conversation.\n",
        "        self.chat_conversation.append(('ai', response))\n",
        "        # Return the chain response to the user for viewing.\n",
        "        return response\n",
        "\n",
        "    # Clear conversation history.\n",
        "    def clear(self):\n",
        "        self.chat_conversation = []"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a5fe594-f3af-40a7-8375-54fd8b7d545a",
      "metadata": {
        "id": "3a5fe594-f3af-40a7-8375-54fd8b7d545a"
      },
      "source": [
        "Let's try it out with one of the system messages defined above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2a019fd-7b29-4e88-b02b-870c2dd52ddb",
      "metadata": {
        "id": "d2a019fd-7b29-4e88-b02b-870c2dd52ddb"
      },
      "outputs": [],
      "source": [
        "brief_chatbot = ChatbotWithRole(llm, system_message=brief_chatbot_system_message)\n",
        "curious_chatbot = ChatbotWithRole(llm, system_message=curious_chatbot_system_message)\n",
        "increased_vocabulary_chatbot = ChatbotWithRole(llm, system_message=increased_vocabulary_system_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c93dacb-2730-45f1-9f7e-c859dd2fa808",
      "metadata": {
        "id": "9c93dacb-2730-45f1-9f7e-c859dd2fa808",
        "outputId": "0e3124dc-292c-41f6-ef7b-e5279da10ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wake up 15 minutes earlier than usual, stretch, drink water, meditate for 5 minutes, and exercise for 10 minutes.\n"
          ]
        }
      ],
      "source": [
        "print(brief_chatbot.chat(\"What would you consider a good morning routine?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36ae27fb-6d0c-4a81-8a55-24cafacbc492",
      "metadata": {
        "id": "36ae27fb-6d0c-4a81-8a55-24cafacbc492",
        "outputId": "1f0f3f82-dee0-4c49-f74b-ac8acfc41587"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A good morning routine! It's like the foundation of a beautiful day, don't you think? I'm curious, what do you think makes a morning routine \"good\"? Is it about waking up early, or is it more about setting a positive tone for the day?\n",
            "\n",
            "For me, I think a good morning routine is all about finding a balance between relaxation and productivity. I'd love to hear about your morning habits! Do you have a consistent routine, or do you like to mix things up?\n",
            "\n",
            "Some people swear by meditation and yoga, while others prefer a good cup of coffee and a quick workout. And then there are those who just want to hit the snooze button and roll out of bed (no judgment here!).\n",
            "\n",
            "What about you? What's your morning routine like?\n"
          ]
        }
      ],
      "source": [
        "print(curious_chatbot.chat(\"What would you consider a good morning routine?\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58954c41-5cd8-43d1-bd81-7382972ec8c9",
      "metadata": {
        "id": "58954c41-5cd8-43d1-bd81-7382972ec8c9",
        "outputId": "b10ebd66-7ff5-486c-f261-60049fb48e1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "A salubrious morning routine, one that sets the tone for a diurnal existence replete with productivity and vitality, would likely involve a judicious combination of invigorating activities. I would recommend commencing the day with a refreshing ablution, followed by a brisk and invigorating exercise routine, such as a brisk jog or a series of dynamic yoga poses.\n",
            "\n",
            "Subsequently, a nutritious and wholesome breakfast, replete with a balanced amalgam of complex carbohydrates, lean protein, and healthy fats, would be an excellent way to satiate the pangs of hunger and provide sustained energy throughout the morning. A stimulating cup of coffee or tea, imbued with the essence of aromatic spices, would also be a welcome addition to this morning ritual.\n",
            "\n",
            "Furthermore, a brief period of mental preparation, involving a meditative or contemplative practice, such as mindfulness or journaling, would be an excellent way to clarify one's thoughts, focus the mind, and set intentions for the day ahead. This would enable one to approach the challenges and opportunities of the day with a sense of purpose, clarity, and equanimity.\n",
            "\n",
            "Ultimately, a good morning routine should be tailored to the individual's unique needs, preferences, and circumstances, and should be designed to foster a sense of well-being, productivity, and joy. By incorporating a judicious combination of physical, mental, and emotional nourishment, one can set the stage for a day that is both fulfilling and efficacious.\n"
          ]
        }
      ],
      "source": [
        "print(increased_vocabulary_chatbot.chat(\"What would you consider a good morning routine?\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cdccb5a-c6a9-4280-9994-9f6e28dd25ac",
      "metadata": {
        "id": "2cdccb5a-c6a9-4280-9994-9f6e28dd25ac"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a60cf7d2-fab9-4f76-8fce-d479d043e35a",
      "metadata": {
        "id": "a60cf7d2-fab9-4f76-8fce-d479d043e35a"
      },
      "source": [
        "## Gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d8d91b2-dd99-4a35-ae29-149e25e7d9fc",
      "metadata": {
        "id": "1d8d91b2-dd99-4a35-ae29-149e25e7d9fc"
      },
      "source": [
        "_\"Gradio is the fastest way to demo your machine learning model with a friendly web interface so that anyone can use it, anywhere!\"_\n",
        "\n",
        "If you find yourself building chatbots, especially for prototypes or even personal use chatbots, you might consider [Gradio](https://www.gradio.app/), which makes it easy to setup a pleasant chat interface, including in Jupyter environments.\n",
        "\n",
        "Pass a `chatbot` instance (created with either the `Chatbot` class or `ChatbotWithRole` class) into the following `create_chatbot_interface` function and have a conversation. If you're interested, check out [chat_helpers/gradio_interface.py](chat_helpers/gradio_interface.py) for the source code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca1c1749-3d41-424f-ae70-f4661ad8964d",
      "metadata": {
        "id": "ca1c1749-3d41-424f-ae70-f4661ad8964d"
      },
      "outputs": [],
      "source": [
        "from chat_helpers.gradio_interface import create_chatbot_interface"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6af802b-ef30-4bd8-92e5-61e0b1019153",
      "metadata": {
        "id": "a6af802b-ef30-4bd8-92e5-61e0b1019153",
        "outputId": "018c80d4-857f-4431-9129-9c6d143a1d9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on local URL:  http://127.0.0.1:7860\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/site-packages/gradio/analytics.py:106: UserWarning: IMPORTANT: You are using gradio version 4.44.0, however version 4.44.1 is available, please upgrade. \n",
            "--------\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running on public URL: https://ea03ae2f4250a4b968.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://ea03ae2f4250a4b968.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "app = create_chatbot_interface(curious_chatbot)\n",
        "app.launch(share=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd931125-607d-4346-9a6a-8359cb64e384",
      "metadata": {
        "id": "bd931125-607d-4346-9a6a-8359cb64e384"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e0180ae-c224-41ca-8111-e2e0091e123a",
      "metadata": {
        "id": "0e0180ae-c224-41ca-8111-e2e0091e123a"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5cb5233-a5f4-491c-9db9-ca7d5a31bb95",
      "metadata": {
        "id": "a5cb5233-a5f4-491c-9db9-ca7d5a31bb95"
      },
      "source": [
        "In this notebook you learned how to leverage a new message type, a placeholder message, to create chatbots capable of retaining conversation history.\n",
        "\n",
        "This was the final notebook in this section focused primarily on the explicit use of chat message types to benefit your LLM-based application code, and you learned a variety of techniques in addition to managing conversation history, like few-shot prompting, utilizing the system message, and performing chain-of-though prompting.\n",
        "\n",
        "In this next section you will focus your attention on using a variety of prompt engineering techniques to enable your LLM-based applications to generated structured data, a powerful capability that unlocks the ability of your LLM-based applications to interact more immediately with downstream code, and opens incredible possibilities for using LLMs to tag and anaylyze large collections of textual data."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}